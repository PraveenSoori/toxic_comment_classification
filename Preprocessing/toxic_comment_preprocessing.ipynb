{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toxic Comment Classification with BERT\n",
        "\n",
        "This notebook implements a multi-label toxic comment classifier using BERT, PyTorch, and Hugging Face Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n",
        "Run this cell only once to install required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install pandas scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('train.csv', engine='python')\n",
        "print(df.head())\n",
        "print(df.shape)\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Clean Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
        "    text = re.sub(r'\\n', ' ', text)      # remove newlines\n",
        "    text = re.sub(r'[^A-Za-z0-9 ]+', '', text)  # remove special chars\n",
        "    return text.lower()\n",
        "\n",
        "df['comment_text'] = df['comment_text'].astype(str).apply(clean_text)\n",
        "print(df['comment_text'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Labels and Convert to Numeric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "for col in labels:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train-Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Validation shape:\", val_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Load BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Test tokenization\n",
        "sample = train_df['comment_text'].iloc[0]\n",
        "tokens = tokenizer.encode_plus(\n",
        "    sample,\n",
        "    max_length=128,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    add_special_tokens=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "print(tokens['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Dataset Class for PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToxicCommentsDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len, labels):\n",
        "        self.df = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.df.iloc[index]['comment_text']\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        targets = torch.tensor(self.df.iloc[index][self.labels].values, dtype=torch.float)\n",
        "        return {\n",
        "            'input_ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(inputs['token_type_ids'], dtype=torch.long),\n",
        "            'targets': targets\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Initialize DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = ToxicCommentsDataset(train_df.reset_index(drop=True), tokenizer, MAX_LEN, labels)\n",
        "val_dataset = ToxicCommentsDataset(val_df.reset_index(drop=True), tokenizer, MAX_LEN, labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Quick check\n",
        "data = next(iter(train_loader))\n",
        "print(data['input_ids'].shape)  # [BATCH_SIZE, MAX_LEN]\n",
        "print(data['targets'].shape)    # [BATCH_SIZE, 6]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
